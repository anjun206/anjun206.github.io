1차 프로토타입

그냥 stt하고 번역하고 tts후 합쳐서 아웃풋

결과:
번역후 늘어난 문장 그냥 tts해서 원본 영상보다 시간 +
그치만 그냥 이어붙이는 방식이라 시간 오버
-> 한번 깨지면 이후는 다 망가짐

2차 프로토타입

tts후 합치는 과정에서 이어붙이는 것이 아닌 절대좌표로 기록해 이후 세그먼트에 영향안가게

실패

start, end 메타데이터에 재기록해서 시도
한번에 stt->번역->tts 대신에 각 단계별 수정가능하게 변경

실패 후 문제점 알게됨

문제점:
원문 세그먼트 길이랑 미스매칭
언어별 문법등에 의한 이슈

해결방안:
세그먼트 범위 넓혀 다른 세그먼트랑 합쳐서 번역


문제점2:
세그먼트를 합쳐버리면 tts시에 몇초부터 몇초까지 말해야 하는지 어려움

해결방안2:
공백도 세그먼트 범위에 저장해둬야함
그리고 번역 이후 다시 쪼개야할듯함

그치만... 어떻게?

일단 stt 과정에서 공백도 따로 기록해야함

합치고 번역후, 병합하느라 커져있는 블록 tts로 한번 돌려 시간 측정

원본 영상 시간과 비교 (공백 제외)
병합 이전 세그먼트들의 시간에 비례하여 tts분배


문제점3:
커져있는 블록 tts 한번 돌렸는데 이를 쪼개고 나서의 시간과 문맥 조절 어려움

단순 비례로 쪼갤시 말하다 끊길 가능성 농후

단어 길이로 쪼개기?

해결방안3:
아직 잘 모르겠음
LLM 시켜서 원 문장과 비교시켜 자연스레 쪼개기?
...
스마트컷이란 방법 추천받음

스마트컷:
오디오만 다룬다
병합시 병합 이전 슬롯들과 내부 공백 저장
병합된 번역문을 TTS로 한번 실행
이 합성된 오디오를 내부 공백의 무음점을 경계로 자름
무음없으면 그대로 하되 단조 증가 보정? 모노토닉 보정? 이런거 해줘야함
이후, 잘린 조각 원본 시간과 비교해 늘리거나 줄임
마지막으로 내부 갭까지 삽입해 리듬 복원



첫 레퍼런스 = 침착맨 ai 더빙 = 팀버 사이트

팀버 핵심:
내부 구조는 비공개지만, 통상 **피치·에너지·길이(발화 길이/폰 지속시간)** 를 예측·주입하는 **Variance Adaptor(FastSpeech 2 계열)** 와, 화자 음색(클로닝)을 결합해 의미는 번역, 억양·톤은 원음 스타일로 합성

레퍼런스:
https://perso.ai/

https://www.timbr.studio/